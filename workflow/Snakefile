###################################
############ Libraries ############
###################################

import pathlib

################################
############ Config ############
################################

segment_length_threshold = config["segmentLengthThreshold"]

deploy_offline = config["deploy_offline"]

scripts_dir = config["scripts_dir"]

reference = config.get('reference', None)

################################
########## Variables ###########
################################

bwa_index_suffices = ["amb", "ann", "bwt", "pac", "sa"]

################################
########## Functions ###########
################################

def get_script_path(*args):
    out = pathlib.Path(scripts_dir).joinpath(*args).resolve(strict=True)
    return out.as_posix()

###################################
############ Rules ################
###################################

include: 'rules/sample_table.smk'
include: 'rules/collect_output.smk'

# if reference:
include: 'rules/evaluate.smk'


#####################################
############ Offline/Online #########
#####################################

if deploy_offline:
    conda_r_env = None
    singularity_r_env = config["singularity_Renv"]
    if not pathlib.Path(singularity_r_env).is_file():
        raise FileNotFoundError('R Singularity environment not found.')
else:
    conda_r_env = "envs/env_Renv2.yaml"
    singularity_r_env = None

###############################
############ All ##############
###############################

if reference:
    ALL_OUTPUT.append(expand("reference_alignments/{ref_name}/{sample}_{ref_name}_ref-aln.paf", sample=SAMPLES, ref_name=ref_name))

rule all:
    input: ALL_OUTPUT

################################################
############ Unmerged SS Processing ############
################################################

# Unmerged reads are aligned in bwa mem paired end mode, and alignments are
# used for the initial clustering step which assigns
# unitig chromosome and orientation, and calls library strand state
def sample2ss(wildcards):
    out = MAP_SAMPLE_TO_INPUT[wildcards.sample]['strandseq_pairs'][wildcards.lib][wildcards.pair]
    return out

rule unzip_ss:
    input: sample2ss
    output: temp("ss/unmerged/{sample}/{lib}_{pair}.fasta")
    conda:'envs/env_cl.yaml'
    log: "log/unzip_ss_{sample}_{lib}_{pair}.log"
    shell:
        '''
        (time bioawk -c fastx '{{print \">\"$name; print $seq}}' <(cat {input}) > {output}) > {log} 2>&1
        '''

# TODO I don't think this step is necessary anymore
rule add_ss_libname_unmerged:
    input: "ss/unmerged/{sample}/{lib}_{pair}.fasta"
    output: temp("ss/unmerged/{sample}/{lib}_{pair}.renamed.fasta")
    conda:'envs/env_cl.yaml'
    log: "log/add_ss_libname_{sample}_{lib}_{pair}.log"
    shell:
        '''
        (bioawk -c fastx -v libname={wildcards.lib} '{{print \">\"$name"_"libname; print $seq}}' <(cat {input}) > {output}) > {log} 2>&1
        '''

rule homopolymer_compress_unmerged_ss:
    input: "ss/unmerged/{sample}/{lib}_{pair}.renamed.fasta"
    output: "ss/unmerged/{sample}/{lib}_{pair}.homopolymer-compressed.fasta"
    params:
        script=get_script_path('python','homopolymer_compress_fasta.py')
    conda:'envs/env_pyenv.yaml'
    log: "log/compress_unmerged_ss_{sample}_{lib}_{pair}.log"

    shell:
        '''
        (python3 {params.script} \\
        --input {input} \\
        --output {output}) > {log} 2>&1
        '''


##############################################
############ Merged SS Processing ############
##############################################
# Merged SS reads are used with bwa fastmap, for longer exact matches.
# How useful merging actually is for that step is untested, but it feels like
# the right thing to do as fastmap does not have a paired alignment mode

# TODO check the name of the merged reads, does it simply take the name of the first read in the pair?


rule pear_merge_mates:
    input:
        fq1=lambda wildcards: MAP_SAMPLE_TO_INPUT[wildcards.sample]['strandseq_pairs'][wildcards.lib]['file1'],
        fq2=lambda wildcards: MAP_SAMPLE_TO_INPUT[wildcards.sample]['strandseq_pairs'][wildcards.lib]['file2']
    output:
        "ss/merged/{sample}/{lib}.assembled.fastq",
        "ss/merged/{sample}/{lib}.discarded.fastq",
        "ss/merged/{sample}/{lib}.unassembled.forward.fastq",
        "ss/merged/{sample}/{lib}.unassembled.reverse.fastq"
    conda:'envs/env_cl.yaml'
    resources:
        mem_mb = lambda wildcards, attempt: 4096 * attempt,
        walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
    log: "log/pear_merge_mates_{sample}_{lib}.log"
    shell: "(pear -f {input.fq1} -r {input.fq2} -t 101 -o ss/merged/{wildcards.sample}/{wildcards.lib}) > {log} 2>&1"

rule concat_assembled_with_first_pair_of_unassembled:
    input:
        "ss/merged/{sample}/{lib}.assembled.fastq",
        "ss/merged/{sample}/{lib}.unassembled.forward.fastq"
    output: temp("ss/merged/{sample}/{lib}.combined.fasta")
    conda:'envs/env_cl.yaml'
    log: "log/concat_merged_with_first_unmerged_{sample}_{lib}.log"
    shell: "(time bioawk -c fastx '{{print \">\"$name; print $seq}}' <(cat {input}) > {output}) > {log} 2>&1"

rule add_ss_libname_merged:
    input: "ss/merged/{sample}/{lib}.combined.fasta"
    output: temp("ss/merged/{sample}/{lib}.combined.renamed.fasta")
    conda:'envs/env_cl.yaml'
    # log: "log/add_ss_libname_{sample}_{lib}.log"
    shell:
        '''
        bioawk -c fastx -v libname={wildcards.lib} '{{print \">\"$name"_"libname; print $seq}}' <(cat {input}) > {output}
        '''

rule homopolymer_compress_merged_ss:
    input: "ss/merged/{sample}/{lib}.combined.renamed.fasta" # concat_assembled_with_first_pair_of_unassembled
    output: "ss/merged/{sample}/{lib}.combined.homopolymer-compressed.fasta"
    params:
        script=get_script_path('python','homopolymer_compress_fasta.py')
    conda:'envs/env_pyenv.yaml'
    # log: "log/compress_ss_{sample}_{lib}.log"
    shell:
        '''
        python3 {params.script} \\
        --input {input} \\
        --output {output}
        '''

#######################################
############ Index Unitigs ############
#######################################
rule gfa_to_fasta:
    input: lambda wildcards: MAP_SAMPLE_TO_INPUT[wildcards.sample]['gfa']
    output: "fasta/{sample}/{sample}_unitigs-hpc.fasta"
    threads: 2
    log: "log/gfa_to_fasta_{sample}.log"
    shell:
        '''
        (time grep S {input} | awk '{{print \">\"$2\"\\n\"$3}}' > {output}) > {log} 2<&1
        '''

rule bwa_index_unitigs:
    input: "fasta/{sample}/{sample}_unitigs-hpc.fasta" # gfa_to_fasta
    output: expand("fasta/{{sample}}/{{sample}}_unitigs-hpc.fasta.{bwa_index_suffix}", bwa_index_suffix=bwa_index_suffices)
    conda:'envs/env_cl.yaml'
    resources:
        mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
        walltime = lambda wildcards, attempt: f'{8 + attempt*attempt:02}:59:00'
    log: "log/bwa_index_unitigs_{sample}.log"
    shell: "(time bwa index {input}) > {log} 2>&1"

#####################################################################
############ bwa mem Unmerged SS Reads to Assembly Unitigs ##########
#####################################################################

rule bwa_align_unmerged_compressed_ss_to_unitigs:
    input:
        unitigs="fasta/{sample}/{sample}_unitigs-hpc.fasta", # gfa_to_fasta
        unitigs_index=expand("fasta/{{sample}}/{{sample}}_unitigs-hpc.fasta.{bwa_index_suffix}", bwa_index_suffix=bwa_index_suffices), # bwa_index_unitigs
        mate1="ss/unmerged/{sample}/{lib}_file1.homopolymer-compressed.fasta",
        mate2="ss/unmerged/{sample}/{lib}_file2.homopolymer-compressed.fasta"
    output: temp("temp_bwa_alignments/mem/{sample}/{lib}.bam")
    threads: 6
    conda:'envs/env_cl.yaml'
    resources:
        mem_mb = lambda wildcards, attempt: 1024 * 16 * attempt,
        walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
    log:
        bwa="log/bwa_align_unmerged_compressed_ss_to_unitigs_bwa_{sample}_{lib}.log",
        samtools="log/bwa_align_unmerged_compressed_ss_to_unitigs_samtools_{sample}_{lib}.log"
    shell:
        '''
        bwa mem -t {threads} -R "@RG\\tID:{wildcards.lib}" -v 2 {input.unitigs} {input.mate1} {input.mate2} 2> {log.bwa} | samtools view -b -F 2304 /dev/stdin > {output} 2> {log.samtools}
        '''

rule bwa_sort_unmerged_compressed_ss_alignments:
    input:  "temp_bwa_alignments/mem/{sample}/{lib}.bam" # bwa_align
    output: temp("bwa_alignments/mem/{sample}/{lib}.bam")
    conda:'envs/env_cl.yaml'
    resources:
        mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
        walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
    log: "log/bwa_sort_unmerged_{sample}_{lib}.log"
    shell:
        '''
        (time samtools sort -o {output} {input}) > {log} 2>&1
        '''

rule unmerged_mark_duplicates:
    input:  "bwa_alignments/mem/{sample}/{lib}.bam" # bwa_sort
    output: "bwa_alignments/mem/{sample}/{lib}.mdup.bam"
    conda:'envs/env_cl.yaml'
    resources:
        mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
        walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
    log: "log/unmerged_mark_duplicates_{sample}_{lib}.log"
    shell:
        '''
        (time sambamba markdup {input} {output}) > {log} 2>&1
        '''

# DO I still need this rule from Maryam? Does it make filtering and import of the Bams in the R program easier?
rule unmerged_bwa_index:
    input:  "bwa_alignments/mem/{sample}/{lib}.mdup.bam" # mark_duplicates
    output: "bwa_alignments/mem/{sample}/{lib}.mdup.bam.bai"
    conda:'envs/env_cl.yaml'
    resources:
        mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
        walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
    log: "log/unmerged_bwa_index_{sample}_{lib}.log"
    shell:
        '''
        (time samtools index {input}) > {log} 2>&1
        '''

#############################################################
############ fastmap merged Strand-seq reads ################
#############################################################

rule fastmap_ss_reads_to_unitigs:
	input:
		unitigs="fasta/{sample}/{sample}_unitigs-hpc.fasta",# gfa_to_fasta
		unitigs_index=expand("fasta/{{sample}}/{{sample}}_unitigs-hpc.fasta.{bwa_index_suffix}",bwa_index_suffix=bwa_index_suffices),# bwa_index_unitigs
		SSreads="ss/merged/{sample}/{lib}.combined.homopolymer-compressed.fasta",
	output: "bwa_alignments/fastmap/{sample}/{lib}_maximal_unique_exact_match.tsv" # fastmap doesn't output bam
	threads: 2
	conda: 'envs/env_cl.yaml'
	resources:
		mem_mb=lambda wildcards, attempt: 1024 * 8 * attempt,
		walltime=lambda wildcards, attempt: f'{attempt * attempt:02}:59:00'
	log: "log/map_SS_reads_to_unitigs_{sample}_{lib}.log"
	shell: "(bwa fastmap -w 1 -l 75 {input.unitigs} {input.SSreads} > {output}) > {log} 2>&1"

###################################################
############ Acrocentric Tangle Removal ###########
###################################################

# Copying the assembly to the working directory is a convenience to get around having to manage paths related to the singularity environment

# rule copy_assembly_to_wd:
#     input:
#         lambda wildcards: MAP_SAMPLE_TO_INPUT[wildcards.sample]['gfa']
#     output:
#         temp("{sample}/assembly.gfa")
#     shell: 'cp {input} {output}'

rule remove_acrocentric_tangle:
    input: lambda wildcards: MAP_SAMPLE_TO_INPUT[wildcards.sample]['gfa']
    output: gfa = 'gfa/gfa/{sample}_exploded.gfa',
            ccs = 'gfa/ccs/{sample}_exploded_ccs.tsv'
    params:
        segment_length_threshold = segment_length_threshold,
        script = get_script_path('R','explode_largest_component.R')
    conda: 'envs/env_Renv.yaml'
    resources:
        mem_mb = lambda wildcards, attempt: 1024 * 4 * attempt,
        walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
    log: "log/remove_acrocentric_tangle_{sample}.log"
    shell:
        '''
        (Rscript --vanilla {params.script} \\
        --gfa {input} \\
        --output-gfa {output.gfa} \\
        --output-ccs {output.ccs} \\
        --segment-length-threshold {params.segment_length_threshold}) > {log} 2>&1
        '''


###############################################################
####################### Mashmap Homology ######################
###############################################################

rule mashmap_assembly_to_self:
    input:  "fasta/{sample}/{sample}_unitigs-hpc.fasta"
    output: "homology/{sample}/{sample}_mashmap.paf"
    params:
        percent_identity=95,
        min_alignment_size=10000,
        kmer_size=24 # 16 ~ mashmap2 default, 19 ~ mashmap3 default
    conda:'envs/env_mashmap.yaml'
    resources:
        mem_mb = lambda wildcards, attempt: 1024 * 24 * attempt,
        walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
    threads: 8
    log: "log/mashmap_{sample}.log"
    benchmark: "benchmark/mashmap_{sample}.benchmark"
    shell:
        '''
        mashmap -r {input} -q {input} -t {threads} -f none --pi {params.percent_identity} -s {params.min_alignment_size} -k {params.kmer_size} -o {output} > {log} 2>&1
        '''

# rule filter_mashmap:
#     input:  "homology/{sample}/{sample}_mashmap.paf"
#     output: "homology/{sample}/{sample}_mashmap_unitig-matches.tsv"
#     conda:'envs/env_cl.yaml'
#     # log: "log/mashmap_filter_{sample}.log"
#     shell:
#         '''
#         cat {input} |awk '{{if ($NF > 99 && $4-$3 > 500000 && $1 != $6) print $1"\t"$6}}'|sort |uniq > {output}
#         '''
#

################################################
############ bubbleGun Homology ############
################################################

rule simplify_assembly:
    input:
        assembly="gfa/gfa/{sample}_exploded.gfa",
        # clusters="clustering_orientation_strandstate/{sample}/unitig_clusters.tsv"
    output: "gfa/gfa/{sample}_exploded_simplified.gfa"
    params:
        segment_length_threshold = segment_length_threshold,
        script=get_script_path('python','simplify_gfa.py')
    conda:'envs/env_gfa.yaml'
    resources:
        mem_mb = lambda wildcards, attempt: 1024 * 16 * attempt,
        walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
    log: "log/simplify_assembly_{sample}.log"
    shell:
        '''
        (python3 {params.script} \\
        --input {input.assembly} \\
        --output {output} \\
        --segment-length-threshold {params.segment_length_threshold}) > {log} 2>&1
        '''

rule detect_bubbles:
    input: "gfa/gfa/{sample}_exploded_simplified.gfa"
    output: "homology/{sample}/{sample}_exploded_simplified_bubblegun.json"
    conda:'envs/env_bubblegun.yaml'
    resources:
        mem_mb = lambda wildcards, attempt: 1024 * 8 * attempt,
        walltime = lambda wildcards, attempt: f'{1 + attempt*attempt:02}:59:00'
    log: "log/detect_bubbles_assembly_{sample}.log"
    shell: "(time BubbleGun -g {input} bchains --only_simple --bubble_json {output})> {log} 2>&1"


################################################
############ combine Homology ############
################################################

rule combine_homology:
    input:
        mashmap="homology/{sample}/{sample}_mashmap.paf",
        bubblegun="homology/{sample}/{sample}_exploded_simplified_bubblegun.json",
        gfa="gfa/gfa/{sample}_exploded_simplified.gfa"
    output: "homology/{sample}/{sample}_combined_mashmap_bubblegun_homology.tsv"
    params:
        script = get_script_path('R','combine_homology.R')
    conda: 'envs/env_Renv.yaml'
    resources:
        mem_mb = lambda wildcards, attempt: 1024 * 4 * attempt,
        walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
    log: "log/combine_homology_{sample}.log"
    shell:
        '''
        (Rscript --vanilla {params.script} \\
        --mashmap {input.mashmap} \\
        --bubblegun {input.bubblegun} \\
        --gfa {input.gfa} \\
        --output {output}) > {log} 2>&1
        '''

#######################################
############ Phase Unitigs ############
#######################################


def sample2mem(wildcards, ext='bam'):

    out = expand("bwa_alignments/mem/{{sample}}/{lib}.mdup."+ext,
        lib=MAP_SAMPLE_TO_INPUT[wildcards.sample]['strandseq_libs'])
    return out

def sample2fastmap(wildcards):
    out = expand("bwa_alignments/fastmap/{{sample}}/{lib}_maximal_unique_exact_match.tsv",
        lib=MAP_SAMPLE_TO_INPUT[wildcards.sample]['strandseq_libs'])
    return out

rule count_haplotype_markers:
    input:
        bam=lambda wildcards: sample2mem(wildcards, 'bam'),
        bai=lambda wildcards: sample2mem(wildcards, 'bam.bai'),
        fastmap=sample2fastmap,
        connected_components='gfa/ccs/{sample}_exploded_ccs.tsv',
        homology="homology/{sample}/{sample}_combined_mashmap_bubblegun_homology.tsv"
    output:
        "haplotype_marker_counts/{sample}_haplotype_marker_counts.csv"
    params:
        segment_length_threshold=segment_length_threshold,
        script = get_script_path('R','clustering_orient_strandstate.snakemake.R')
    singularity: singularity_r_env
    conda: conda_r_env
    resources:
        mem_mb = lambda wildcards, attempt: 1024 * 12 * attempt,
        walltime = lambda wildcards, attempt: f'{attempt*attempt:02}:59:00'
    threads: 1
    log: "log/count_haplotype_markers_{sample}.log"
    shell:
        '''
        (Rscript --vanilla {params.script} \\
        --mem-alignment-bams {input.bam} \\
        --fastmap-alignments {input.fastmap} \\
        --homology {input.homology} \\
        --connected-components {input.connected_components} \\
        --segment-length-threshold {params.segment_length_threshold} \\
        --threads {threads} \\
        --output {output}) > {log} 2>&1
        '''


#######################################
############ Rukki ############
#######################################


rule add_coverage_to_gfa:
    input:
        gfa=lambda wildcards: MAP_SAMPLE_TO_INPUT[wildcards.sample]['gfa'],
        coverage=lambda wildcards: MAP_SAMPLE_TO_INPUT[wildcards.sample]['coverage']
    output:
        temp('{sample}_gfa_with_coverage.gfa')
    conda: 'envs/env_verkko.yaml'
    resources:
        mem_mb=lambda wildcards, attempt: 1024 * 8 * attempt,
        walltime=lambda wildcards, attempt: f'{attempt * attempt:02}:59:00'
    log: "log/add_coverage_to_gfa_{sample}.log"
    benchmark: "benchmark/add_coverage_to_gfa_{sample}.benchmark"
    shell:
        '''
        ($CONDA_PREFIX/lib/verkko/scripts/inject_coverage.py --allow-absent {input.coverage} {input.gfa} > {output}) > {log} 2>&1
        '''

rule convert_markers_to_tsv:
    input: "haplotype_marker_counts/{sample}_haplotype_marker_counts.csv"
    output: temp("haplotype_marker_counts/{sample}_haplotype_marker_counts.tsv")
    shell:
        '''
           tail -n +2 {input} | awk -F',' '{{OFS="\t"; $1=$1; print}}' > {output}
        '''


rule run_rukki:
    input:
        graph='{sample}_gfa_with_coverage.gfa',
        markers="haplotype_marker_counts/{sample}_haplotype_marker_counts.tsv"
    output:
        tsv="rukki/{sample}/{sample}_rukki_paths.tsv",
        gaf="rukki/{sample}/{sample}_rukki_paths.gaf",
        initial_annotation="rukki/{sample}/{sample}_out_init_ann.csv",
        refined_annotation='rukki/{sample}/{sample}_out_refined_ann.csv',
        final_annotation='rukki/{sample}/{sample}_out_final_ann.csv'
    conda: 'envs/env_verkko.yaml'
    resources:
        mem_mb=lambda wildcards, attempt: 1024 * 12 * attempt,
        walltime=lambda wildcards, attempt: f'{attempt * attempt:02}:59:00'
    log: "log/run_rukki_{sample}.log"
    benchmark: "benchmark/run_rukki_{sample}.benchmark"
    shell:
        '''
        params=""
        params="$params --init-assign {output.initial_annotation}"
        params="$params --refined-assign {output.refined_annotation}"
        params="$params --final-assign {output.final_annotation}"
        params="$params --hap-names haplotype1,haplotype2"

        # Minimal number of parent-specific markers required for assigning parental group to a node [default: 10]
        params="$params --marker-cnt 5" 

        # Require at least (node_length / <value>) markers within the node for parental group assignment [default: 10000]
        params="$params --marker-sparsity 5000000" 

        # Sets minimal marker excess for assigning a parental group to <value>:1 [default: 5]
        params="$params --marker-ratio 3" 

        # Longer nodes are unlikely to be spurious and likely to be reliably assigned based on markers (used in HOMOZYGOUS node labeling) [default: 200000]
        params="$params --trusted-len 250000"
        
        # Require at least (node_length / <value>) markers for assigning ISSUE label (by default == marker_sparsity, will typically be set to a value >= marker_sparsity)
        # params="$params --issue-sparsity 10000" 

        # Require primary marker excess BELOW <value>:1 for assigning ISSUE label. Must be <= marker_ratio (by default == marker_ratio)
        params="$params --issue-ratio 1."

        # Try to fill in small ambiguous bubbles
        params="$params --try-fill-bubbles"

        # Bubbles including a longer alternative sequence will not be filled [default: 50000]
        params="$params --fillable-bubble-len 500000"

        # Bubbles with bigger difference between alternatives' lengths will not be filled [default: 200]
        params="$params --fillable-bubble-diff 10000"

        # Longer nodes are unlikely to represent repeats, polymorphic variants, etc (used to seed and guide the path search) [default: 500000]
        params="$params --solid-len 500000" 
        
        # Solid nodes with coverage below <coeff> * <weighted mean coverage of 'solid' nodes> can not be classified as homozygous. 0. disables check [default: 1.5]
        params="$params --solid-homozygous-cov-coeff 1.1"

        # Sets minimal marker excess for assigning a parental group of solid nodes to <value>:1. Must be <= marker_ratio (by default == marker_ratio)
        params="$params --solid-ratio 1.5"

        # Assign tangles flanked by solid nodes from the same class, # Allow dead-end nodes in the tangles
        params="$params --assign-tangles --tangle-allow-deadend"

        ($CONDA_PREFIX/lib/verkko/bin/rukki trio -g {input.graph} -m {input.markers}              -p {output.tsv} $params) > {log} 2>&1
        ($CONDA_PREFIX/lib/verkko/bin/rukki trio -g {input.graph} -m {input.markers} --gaf-format -p {output.gaf} $params) >> {log} 2>&1
        '''
